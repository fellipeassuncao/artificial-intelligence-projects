{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLASSIFICAÇÃO DE FAKE NEWS USANDO DEEP LEARNING",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP - CLASSIFICAÇÃO DE FAKE NEWS USANDO DEEP LEARNING\n",
        "\n"
      ],
      "metadata": {
        "id": "7LSftkX8MJzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesta prática iremos classificar um texto a partir de algoritmos de classificação baseado em Redes Neurais Recorrentes (RNN), em especifico Long Short Term Memory (LSTM). Para resolução do problema de classificação, passaremos por algumas etapas, conforme discutido em nossos estudos.\n",
        "\n",
        "## Por que usar o LSTM para classificação de texto?\n",
        "Podemos classificar textos a partir do Processamento de Linguagem Natural e diferentes Algoritmos de Classificação baseados em Deep Learning como LSTMs e CNNs.\n",
        "\n",
        "Existem muitos algoritmos de classificação clássicos como Árvores de Decisão, Random Forest, SVM, que podem fazer um bom trabalho, então por que usar LSTM para classificação?\n",
        "\n",
        "> Uma boa razão para usar o LSTM é que ele é eficaz na memorização de informações importantes. Se olharmos e outras técnicas de classificação de redes não neurais, elas são treinadas em várias palavras como entradas separadas que são apenas palavras sem significado real como uma frase, e ao prever a classe dará a saída de acordo com as estatísticas e não de acordo com o significado. Isso significa que cada palavra é classificada em uma das categorias.\n",
        "\n",
        "> Isso ocorre de maneira diferente em LSTM. No LSTM, podemos usar uma string de várias palavras para descobrir a classe à qual ela pertence. Isso é muito útil ao trabalhar com Processamento de Linguagem Natural. Se usarmos camadas apropriadas de incorporação e codificação em LSTM, o modelo será capaz de descobrir o significado real na string de entrada e fornecer a classe de saída mais precisa. O código a seguir elaborará a ideia de como a classificação de texto é feita usando LSTM.\n",
        "\n",
        "Para a construção do modelo e testes, você pode baixar os dados em https://www.kaggle.com/c/fake-news/data#"
      ],
      "metadata": {
        "id": "AVAINdM7shUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Referencias\n",
        "> https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
        "\n",
        "> "
      ],
      "metadata": {
        "id": "svFG4uOIs6rI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importar bibliotecas\n"
      ],
      "metadata": {
        "id": "fdCmbno5Mkx4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z7-58Q-MJNl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importar e verificar os dados"
      ],
      "metadata": {
        "id": "2O6LWm4of8hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df=pd.read_csv('/content/drive/MyDrive/PLN/train.csv')\n",
        "\n",
        "'''\n",
        "# Upload from google drive\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "print(\"len(uploaded.keys():\", len(uploaded.keys())) \n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn]))) \n",
        "'''"
      ],
      "metadata": {
        "id": "-lRxJ2KpftwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "kyIz8AwafxVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Drop Nan Values\n",
        "df=df.dropna()\n",
        "## Get the Independent Features\n",
        "X=df.drop('label',axis=1)\n",
        "## Get the Dependent features\n",
        "y=df['label']"
      ],
      "metadata": {
        "id": "zcYwXq4If4sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "K1fEsIUqgHLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "HXkG441ggJAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "Hbj5dnyDgKXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "id": "4s-NKjpTgLkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import Dropout"
      ],
      "metadata": {
        "id": "rRiMLTkjgNKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Vocabulary size\n",
        "voc_size=5000"
      ],
      "metadata": {
        "id": "_OzGgxNGgjBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-hot representation"
      ],
      "metadata": {
        "id": "aYttYwK9glI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages=X.copy()"
      ],
      "metadata": {
        "id": "JpRXtVgbgoJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages['title'][1]"
      ],
      "metadata": {
        "id": "fMxF-BPEgqOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "vRPpv4LyguIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Pre processing"
      ],
      "metadata": {
        "id": "E-EN1S7Tg02_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "-v0jHEWgg0eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Dataset Preprocessing\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "corpus = []\n",
        "for i in range(0, len(messages)):\n",
        "    print(i)\n",
        "    review = re.sub('[^a-zA-Z]', ' ', messages['title'][i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    \n",
        "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)"
      ],
      "metadata": {
        "id": "A_iqTWvrhAM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print corpus content\n",
        "print(corpus)"
      ],
      "metadata": {
        "id": "Sgwu1OdxhTxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot representation\n",
        "onehot_repr=[one_hot(words,voc_size)for words in corpus] \n",
        "onehot_repr"
      ],
      "metadata": {
        "id": "2f9R6mtKhjig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Representation"
      ],
      "metadata": {
        "id": "nDrzC948h2xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_length=20\n",
        "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
        "print(embedded_docs)"
      ],
      "metadata": {
        "id": "Suofnl_Vh6BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_docs[0]"
      ],
      "metadata": {
        "id": "UGG1G3RDh77Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating model"
      ],
      "metadata": {
        "id": "SyJRp9bbiCVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Creating model\n",
        "embedding_vector_features=40\n",
        "model=Sequential()\n",
        "model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "ytJAV-aDiIHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adicionando o Dropout"
      ],
      "metadata": {
        "id": "xNxftXQmIKOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating model\n",
        "embedding_vector_features=40\n",
        "model1=Sequential()\n",
        "model1.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
        "model1.add(Bidirectional(LSTM(100)))\n",
        "model1.add(Dropout(0.5))\n",
        "model1.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "print(model1.summary())"
      ],
      "metadata": {
        "id": "zmet8QoUIEor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embedded_docs), y.shape"
      ],
      "metadata": {
        "id": "fJJJn__jiUrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X_final=np.array(embedded_docs)\n",
        "y_final=np.array(y) "
      ],
      "metadata": {
        "id": "7CMkUn9Fxqlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_final.shape,y_final.shape"
      ],
      "metadata": {
        "id": "4VVzhvpBGMnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X_final,y_final,random_state=42)\n",
        "print(X_train.shape,y_train.shape)\n",
        "print(X_test.shape,y_test.shape)"
      ],
      "metadata": {
        "id": "tOn3p1NYGOHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.15, random_state=42)\n",
        "\n",
        "print(X_train.shape,y_train.shape)\n",
        "print(X_test.shape,y_test.shape)"
      ],
      "metadata": {
        "id": "4GZr2OfQGQX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally Training\n",
        "train_model=model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)"
      ],
      "metadata": {
        "id": "djsogzn8GSY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "metadata": {
        "id": "QV0v0yHMoGr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)"
      ],
      "metadata": {
        "id": "ektlQ1HW_O7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predição e avaliação do modelo\n"
      ],
      "metadata": {
        "id": "wu3ZXgSKGYXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#y_pred1=model.predict_classes(X_test)\n",
        "y_pred1=np.argmax(model.predict(X_test), axis=-1)"
      ],
      "metadata": {
        "id": "2RplvWsrGTu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "2gfsM2HAGVS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_test,y_pred1)"
      ],
      "metadata": {
        "id": "GmMhkkwpGiwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,y_pred1)"
      ],
      "metadata": {
        "id": "SIJWe0bdGkpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test,y_pred1))"
      ],
      "metadata": {
        "id": "6Vq_cl0dGmPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_model.history['accuracy'],'b',label='train_accuracy')\n",
        "plt.plot(train_model.history['val_accuracy'],'r',label='val_accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "IKBDtvgZGr2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pred_val=np.argmax(model.predict(x_val), axis=-1)\n",
        "y_pred1=np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "cm= confusion_matrix(y_test,y_pred1)\n",
        "plot_confusion_matrix(cm, figsize=(5,5))"
      ],
      "metadata": {
        "id": "lk04Ib_xGtlS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
